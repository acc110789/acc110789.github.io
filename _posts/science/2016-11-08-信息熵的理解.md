---
title: 信息熵
---

## {{ page.title }}

### 我的认识

我本科时候的专业是信息安全,当时学了一门课叫信息论。书中上来第一章就给出了一个概念"熵",当时
真的不理解,后来才慢慢地有了一些了解。也认识到了给出这个定义的人---香农的伟大之处。香农能把一个
非常抽象的概念量化,其对信息的理解之深刻!让我感觉到有相似之处的有两个人,一个是哥德尔,哥德尔指出了
数学定理能否被证明的边界条件,他们的成果都是对一个抽象的概念给出了数学的边界。另一个是牛顿,香农和
牛顿的相似之处在于把一个东西用数学描述出来了。

### 熵

什么是熵?熵就是信息的度量,就像长度和直尺的关系。什么是信息?信息就是告诉你一些事情。你原本是
不知道这个事情的,现在我给你说了一些话,你听了我的话之后就知道了。我给你说的话就是信息。举个例子,
我们都知道今年美国总统的决赛是希拉里对阵特朗普,投票结果已经出来了,但是由于你还在办公室勤勤恳恳地
加班,你不知道到底是谁最终当选,这个时候你打开今日头条客户端(这个广告打的完全没有痕迹,老板,给我加钱),有条新闻告诉你特朗普当选美国总统。OK,
这个就是信息。仔细考察这个过程,你原来只是知道总统将从特朗普和希拉里中产生,但是不知道具体是谁,后来你
接收到一个信息,然后你就知道具体是特朗普。信息的作用就是将不确定的东西确定下来,能减少或者完全消除不确定性
的东西都叫信息。好了下面来说熵这个东西,前面说了,熵是信息的度量,这没有错,但是从熵的定义来讲,熵的本质
是不确定性的度量,那信息是消除不确定性的东西,所以熵也可以成为信息的度量。下面举例一段对话的例子:\\
小明说:我现在有一件事情不是很确定,这个事情的不确定度是8(也就是说这个事情的熵是8)。\\
小李说:针对你的问题呢,我给你透露一句话,这句话的熵是5(这就是说这句话所包含信息量是5,你看,信息就这样用熵度量出来了).\\
问小明听了小李的话之后,他对这件事情的不确定度变成了多少?\\
废话,当然是3了,8-5=3不会算?\\
在上面的例子中,我演示了熵的作用,小明听了小李的话之后,对这件事情的不确定度减少了。不确定度减少了有什么用呢?
废话!听了小李的话之后,小明猜测到正确答案的概率明显比之前猜中正确答案的概率大很多了?你还问我有什么用,这就是信息的作用!

下面给出熵的定义(H(X)是指X事件的熵)。

1. 可加性。
2. 连续性。
3. 等概时的单调增函数特性。

重点说一下可加性\\
假设X和Y相互独立,则H(X,Y) = H(X) + H(Y)。这个性质一定要保证,举个例子,我们直观上感觉掷两个骰子的不确定度
应该等于掷第一个骰子的不确定度加上掷第二个骰子的不确定度,如果这个性质不能满足的话,那就和我们直观上感觉到的不一样了。这个是
可加性的一个意义,可加性应该还有一个意义。\\
分层次,比如(1/4,1/4,1/3,1/6)可以先分成(1/2,1/2),然后将第一个1/2里面进一步分成(1/2,1/2)的两个基本事件,第二个1/2里面进一步包含两个基本事件
,其概率是(2/3,1/3),则应该有H(1/4,1/4,1/3,1/6) = H(1/2,1/2) + 1/2\*H(1/3,2/3) + 1/2\*H(1/2,1/2)。做出这样的规则也是符合我们的直观感觉的。
比如明天的天气是刮风或者下雨,概率各为1/2。然后刮风里面刮大风和刮小风的概率各为1/2,下雨里面下大雨的概率是1/3,下小雨的概率是2/3。现在明天天气的
熵是H(1/4,1/4,1/3,1/6)没有问题,然后我告诉你,小明刚刚已经看了天气预报了(假设天气预报预测的天气100%准确),小明会告诉你明天天气具体是刮风还是下雨(
如果告诉你刮风的话,只会告诉你刮风,不会告诉你具体是大风还是小风,下雨也是一样)。但是现在小明还没有告诉你答案,现在我问你待会你知道小明答案后明天天气的熵
是多少?是不是1/2\*H(1/3,2/3) + 1/2\*H(1/2,1/2)?现在应该明白这个可加性的另一重意义了吧。\\
本质上,这个可加性是保证熵这个数学概念必须和我们感觉的那个不确定度或者是信息的度量相一致的关键。可以证明满足上面三个性质
的数学函数在忽略常系数和底数的意义下是唯一的!\\
对了,熵的公式是H(P) = -(p1\*Log(p1) + p2\*Log(p2) + …… + pn\*Log(pn)) 。

### 几个例子

我记得遇到过一个这样的问题,给1000瓶药水,外观一样。里面有一瓶是有毒的,毒性在一周时间内致死
问要在一周时间内测出来具体是哪一瓶水有毒,至少需要多少只老鼠(一个老鼠可以喝多瓶毒药)?\\
答:原问题每一瓶水是毒药的概率都是1/1000,因此,熵为Log(1000)。老鼠的死活为我们提供信息,老鼠
要么死,要么活,因此一只老鼠给我们提供的信息量Log(2),那要多少只老鼠提供的信息总量才能将熵减少到0(也就是真想大白)呢?
很明显,答案是Log(1000)/Log(2),也就是10只老鼠。当然一只老鼠提供的信息量最多是Log(2),在你
实际设计的算法(编码)中,有可能因为你设计的算法问题导致一个老鼠提供的信息小于Log(2),但是熵还是固定的,这就意味着你需要更多的老鼠
才能得出是哪一瓶水有毒,当然在实际情况中,还要其它情况导致老鼠的死亡,比如你选的某只老鼠患有抑郁症
,恰好就在你实验的那一周抑郁死了,或者其它的原因导致死了,这种情况就对应了信道中有噪声(老鼠实际上就是信道中传输的信号),噪声会导致平均每
个信号(每只老鼠)携带的信息量小于Log(2),这时候解决方法就是加冗余,什么校验码,纠错码等知识就跟着牵扯出来了,这里就不详细说了。

还有个问题是这样的,有12个砝码,其中11个质量相同,有一个砝码比其它砝码都重,现在给你一个天平,问
至少要称几次才能找到质量较大的那个砝码?\\
答:很明显,熵是Log(12),天平称一次会产生三个结果,要么左边重,要么左边轻,要么左边不重不轻(平衡),所以
一次称量能提供的信息最多最多是Log(3),所以最少的称量次数是Log(12)/Log(3)。也就是3次。(实际上算出来是2.2还是2.3,但是你不可能
说称2.3次吧,肯定是个整数啊!)

有12个砝码,其中11个质量相同,有一个砝码跟其它砝码质量不一样,现在给你一个天平,问
至少要称几次才能找到特殊的那个砝码?\\
答:这个问题跟上面的有点不一样,12个砝码选一个的熵是Log(12),其次质量不一样,可能是轻,可能是重。利用
可加性的第二重意义H = Log(12) + 12 \* 1/12 \* Log(2) = Log(12) + Log(2) = Log(24)。或者直接考虑
有可能是第一个球重,或者是第一个球轻,或者是第二个球重,或者是第二个球轻……一个24中等可能的情况H = Log(24)。
同样每一个称量最多提供Log(3)的信息,所以最少需要的称量次数是Log(24)/Log(3) 仍然是3。
